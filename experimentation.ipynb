{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 0.3.2 version. Select nrows to a small number when running on huge datasets.\n",
      "output = featurewiz(dataname, target, corr_limit=0.90, verbose=2, sep=',', \n",
      "\t\theader=0, test_data='',feature_engg='', category_encoders='',\n",
      "\t\tdask_xgboost_flag=False, nrows=None, skip_sulov=False, skip_xgboost=False)\n",
      "Create new features via 'feature_engg' flag : ['interactions','groupby','target']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "from Helpers.radiomics_setup import RadiomicsClean, MultiParamProcessor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = [\n",
    "    r\"Data\\UC2_retrospective\\ProCancer_UC2_N4_T2_DL_PZ_radio.csv\",\n",
    "    r\"Data\\UC2_retrospective\\ProCancer_UC2_N4_ADC_DL_PZ_radio.csv\",\n",
    "    r\"Data\\UC2_retrospective\\ProCancer_UC2_N4_DWI_DL_PZ_radio.csv\",\n",
    "]\n",
    "\n",
    "exl = [\"study_uid\",\n",
    "\"number_of_series\",\n",
    "\"provided_by\",\n",
    "\"manufacturer\",\n",
    "\"manufacturer_model_name\",\n",
    "\"pi_rads\"]\n",
    "\n",
    "problematic = r\"D:\\dimza\\EXPERIMENTS\\ProstateCancerClinicalSignificance_UC2\\UC2_prospective_and_retrospective\\Borderline_cases_to_remove.csv\"\n",
    "jenn = r\"Data\\data_characteristics.csv\"\n",
    "\n",
    "mpp = MultiParamProcessor(\n",
    "    csv_paths = csv_paths,\n",
    "    problematic = problematic\n",
    ")\n",
    "mpp.iteration(\n",
    "    suffices=[\"T2\", \"ADC\", \"DWI\"],\n",
    "    columns_to_exclude= exl,\n",
    "    target_csv=jenn\n",
    ")\n",
    "\n",
    "dfs = mpp.get_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retro = pd.concat([dfs[0], dfs[1], dfs[2]], axis = 1)\n",
    "X = retro.drop('Target', axis=1)  # Drop the 'Target' column for X_train\n",
    "y = retro['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helpers import pipelines\n",
    "from Helpers import behave_metrics\n",
    "from Helpers import shap_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hyperparameters = {'C': 1.0, 'penalty': 'l2'}\n",
    "svm_hyperparameters = {'C': 1.0, 'kernel': 'rbf', 'probability':True}\n",
    "xgb_hyperparameters = {'n_estimators': 100}\n",
    "ada_hyperparameters = {'n_estimators': 50}\n",
    "rf_hyperparameters = {'n_estimators': 100}\n",
    "dt_hyperparameters = {'criterion': 'gini'}\n",
    "\n",
    "names = [\"Logistic Regression\", \"SVM\", \"Random Forest\", \"AdaBoost\", \"Decision trees\", \"XGBoost\"]\n",
    "classifiers = [LogisticRegression, SVC, RandomForestClassifier, AdaBoostClassifier, DecisionTreeClassifier, XGBClassifier]\n",
    "hypers = [lr_hyperparameters , svm_hyperparameters, rf_hyperparameters, ada_hyperparameters, dt_hyperparameters, xgb_hyperparameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hyperparameters = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': [None, 'l2'],\n",
    "    'solver': ['lbfgs','liblinear', 'saga','newton-cholesky'],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "svm_hyperparameters = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'probability':[True],\n",
    "}\n",
    "\n",
    "rf_hyperparameters = {\n",
    "    'n_estimators': [2,4,6,10,20,50],\n",
    "    'max_depth': [None, 1,2,3, 4,6,8, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "ada_hyperparameters = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    \"n_jobs\":[-1]\n",
    "}\n",
    "\n",
    "dt_hyperparameters = {\n",
    "    'max_depth': [None, 2,3, 5, 10, 20, 30, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "xgb_hyperparameters = {\n",
    "    'n_estimators': [3,5,10, 50],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [None, 1,2, 3, 6, 10, 20],\n",
    "    'subsample': [0.5, 0.7, 1],\n",
    "    'colsample_bytree': [0.5, 0.7, 1],\n",
    "}\n",
    "hypers = [lr_hyperparameters , svm_hyperparameters, rf_hyperparameters, ada_hyperparameters, dt_hyperparameters, xgb_hyperparameters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV-based on stratified K-Fold split and measurements on each fodl seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiz = FeatureWiz(verbose=1)\n",
      "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
      "        X_test_selected = wiz.transform(X_test)\n",
      "        wiz.features  ### provides a list of selected features ###            \n",
      "        \n",
      "featurewiz has selected 0.6 as the correlation limit. Change this limit to fit your needs...\n",
      "Skipping feature engineering since no feature_engg input...\n",
      "Skipping category encoding since no category encoders specified in input...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "    Loaded train data. Shape = (2924, 3739)\n",
      "    Some column names had special characters which were removed...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "No test data filename given...\n",
      "#######################################################################################\n",
      "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
      "#######################################################################################\n",
      "        No variables were removed since no ID or low-information variables found in data set\n",
      "Removing 0 columns from further processing since ID or low information variables\n",
      "#######################################################################################\n",
      "#####  Searching for Uncorrelated List Of Variables (SULOV) in 3738 features ############\n",
      "#######################################################################################\n",
      "    there are no null values in dataset...\n",
      "    Removing (3094) highly correlated variables:\n",
      "Completed SULOV. 644 features selected\n",
      "Time taken for SULOV method = 116 seconds\n",
      "Finally 644 vars selected after SULOV\n",
      "Converting all features to numeric before sending to XGBoost...\n",
      "    Number of booster rounds = 100\n",
      "            Time taken for regular XGBoost feature selection = 8 seconds\n",
      "            Time taken for regular XGBoost feature selection = 7 seconds\n",
      "            Time taken for regular XGBoost feature selection = 5 seconds\n",
      "            Time taken for regular XGBoost feature selection = 4 seconds\n",
      "            Time taken for regular XGBoost feature selection = 2 seconds\n",
      "    Completed XGBoost feature selection in 2 seconds\n",
      "Selected 417 important features. Too many to print...\n",
      "Total Time taken for featurewiz selection = 141 seconds\n",
      "Output contains a list of 417 important features and a train dataframe\n",
      "    Time taken to create entire pipeline = 150 second(s)\n",
      "Best estimator from grid search: LogisticRegression(C=0.01, class_weight='balanced', solver='liblinear')\n",
      "Best score:0.703276445506923\n",
      "Model trained successfully: LogisticRegression(C=0.01, class_weight='balanced', solver='liblinear')\n",
      "wiz = FeatureWiz(verbose=1)\n",
      "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
      "        X_test_selected = wiz.transform(X_test)\n",
      "        wiz.features  ### provides a list of selected features ###            \n",
      "        \n",
      "featurewiz has selected 0.6 as the correlation limit. Change this limit to fit your needs...\n",
      "Skipping feature engineering since no feature_engg input...\n",
      "Skipping category encoding since no category encoders specified in input...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "    Loaded train data. Shape = (2924, 3739)\n",
      "    Some column names had special characters which were removed...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "No test data filename given...\n",
      "#######################################################################################\n",
      "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
      "#######################################################################################\n",
      "        No variables were removed since no ID or low-information variables found in data set\n",
      "Removing 0 columns from further processing since ID or low information variables\n",
      "#######################################################################################\n",
      "#####  Searching for Uncorrelated List Of Variables (SULOV) in 3738 features ############\n",
      "#######################################################################################\n",
      "    there are no null values in dataset...\n",
      "    Removing (3094) highly correlated variables:\n",
      "Completed SULOV. 644 features selected\n",
      "Time taken for SULOV method = 111 seconds\n",
      "Finally 644 vars selected after SULOV\n",
      "Converting all features to numeric before sending to XGBoost...\n",
      "    Number of booster rounds = 100\n",
      "            Time taken for regular XGBoost feature selection = 7 seconds\n",
      "            Time taken for regular XGBoost feature selection = 6 seconds\n",
      "            Time taken for regular XGBoost feature selection = 5 seconds\n",
      "            Time taken for regular XGBoost feature selection = 3 seconds\n",
      "            Time taken for regular XGBoost feature selection = 2 seconds\n",
      "    Completed XGBoost feature selection in 2 seconds\n",
      "Selected 417 important features. Too many to print...\n",
      "Total Time taken for featurewiz selection = 133 seconds\n",
      "Output contains a list of 417 important features and a train dataframe\n",
      "    Time taken to create entire pipeline = 142 second(s)\n",
      "Model trained successfully: LogisticRegression(C=0.01, solver='liblinear')\n",
      "wiz = FeatureWiz(verbose=1)\n",
      "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
      "        X_test_selected = wiz.transform(X_test)\n",
      "        wiz.features  ### provides a list of selected features ###            \n",
      "        \n",
      "featurewiz has selected 0.6 as the correlation limit. Change this limit to fit your needs...\n",
      "Skipping feature engineering since no feature_engg input...\n",
      "Skipping category encoding since no category encoders specified in input...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "    Loaded train data. Shape = (2924, 3739)\n",
      "    Some column names had special characters which were removed...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "No test data filename given...\n",
      "#######################################################################################\n",
      "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
      "#######################################################################################\n",
      "        No variables were removed since no ID or low-information variables found in data set\n",
      "Removing 0 columns from further processing since ID or low information variables\n",
      "#######################################################################################\n",
      "#####  Searching for Uncorrelated List Of Variables (SULOV) in 3738 features ############\n",
      "#######################################################################################\n",
      "    there are no null values in dataset...\n",
      "    Removing (3094) highly correlated variables:\n",
      "Completed SULOV. 644 features selected\n",
      "Time taken for SULOV method = 113 seconds\n",
      "Finally 644 vars selected after SULOV\n",
      "Converting all features to numeric before sending to XGBoost...\n",
      "    Number of booster rounds = 100\n",
      "            Time taken for regular XGBoost feature selection = 7 seconds\n",
      "            Time taken for regular XGBoost feature selection = 6 seconds\n",
      "            Time taken for regular XGBoost feature selection = 4 seconds\n",
      "            Time taken for regular XGBoost feature selection = 3 seconds\n",
      "            Time taken for regular XGBoost feature selection = 2 seconds\n",
      "    Completed XGBoost feature selection in 2 seconds\n",
      "Selected 417 important features. Too many to print...\n",
      "Total Time taken for featurewiz selection = 135 seconds\n",
      "Output contains a list of 417 important features and a train dataframe\n",
      "    Time taken to create entire pipeline = 144 second(s)\n",
      "Model trained successfully: LogisticRegression(C=0.01, solver='liblinear')\n",
      "wiz = FeatureWiz(verbose=1)\n",
      "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
      "        X_test_selected = wiz.transform(X_test)\n",
      "        wiz.features  ### provides a list of selected features ###            \n",
      "        \n",
      "featurewiz has selected 0.6 as the correlation limit. Change this limit to fit your needs...\n",
      "Skipping feature engineering since no feature_engg input...\n",
      "Skipping category encoding since no category encoders specified in input...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "    Loaded train data. Shape = (2924, 3739)\n",
      "    Some column names had special characters which were removed...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "No test data filename given...\n",
      "#######################################################################################\n",
      "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
      "#######################################################################################\n",
      "        No variables were removed since no ID or low-information variables found in data set\n",
      "Removing 0 columns from further processing since ID or low information variables\n",
      "#######################################################################################\n",
      "#####  Searching for Uncorrelated List Of Variables (SULOV) in 3738 features ############\n",
      "#######################################################################################\n",
      "    there are no null values in dataset...\n",
      "    Removing (3094) highly correlated variables:\n",
      "Completed SULOV. 644 features selected\n",
      "Time taken for SULOV method = 108 seconds\n",
      "Finally 644 vars selected after SULOV\n",
      "Converting all features to numeric before sending to XGBoost...\n",
      "    Number of booster rounds = 100\n",
      "            Time taken for regular XGBoost feature selection = 7 seconds\n",
      "            Time taken for regular XGBoost feature selection = 6 seconds\n",
      "            Time taken for regular XGBoost feature selection = 4 seconds\n",
      "            Time taken for regular XGBoost feature selection = 3 seconds\n",
      "            Time taken for regular XGBoost feature selection = 2 seconds\n",
      "    Completed XGBoost feature selection in 2 seconds\n",
      "Selected 417 important features. Too many to print...\n",
      "Total Time taken for featurewiz selection = 129 seconds\n",
      "Output contains a list of 417 important features and a train dataframe\n",
      "    Time taken to create entire pipeline = 137 second(s)\n",
      "Model trained successfully: LogisticRegression(C=0.01, solver='liblinear')\n",
      "wiz = FeatureWiz(verbose=1)\n",
      "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
      "        X_test_selected = wiz.transform(X_test)\n",
      "        wiz.features  ### provides a list of selected features ###            \n",
      "        \n",
      "featurewiz has selected 0.6 as the correlation limit. Change this limit to fit your needs...\n",
      "Skipping feature engineering since no feature_engg input...\n",
      "Skipping category encoding since no category encoders specified in input...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "    Loaded train data. Shape = (2924, 3739)\n",
      "    Some column names had special characters which were removed...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "No test data filename given...\n",
      "#######################################################################################\n",
      "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
      "#######################################################################################\n",
      "        No variables were removed since no ID or low-information variables found in data set\n",
      "Removing 0 columns from further processing since ID or low information variables\n",
      "#######################################################################################\n",
      "#####  Searching for Uncorrelated List Of Variables (SULOV) in 3738 features ############\n",
      "#######################################################################################\n",
      "    there are no null values in dataset...\n",
      "    Removing (3094) highly correlated variables:\n",
      "Completed SULOV. 644 features selected\n",
      "Time taken for SULOV method = 109 seconds\n",
      "Finally 644 vars selected after SULOV\n",
      "Converting all features to numeric before sending to XGBoost...\n",
      "    Number of booster rounds = 100\n",
      "            Time taken for regular XGBoost feature selection = 7 seconds\n",
      "            Time taken for regular XGBoost feature selection = 6 seconds\n",
      "            Time taken for regular XGBoost feature selection = 4 seconds\n",
      "            Time taken for regular XGBoost feature selection = 3 seconds\n",
      "            Time taken for regular XGBoost feature selection = 2 seconds\n",
      "    Completed XGBoost feature selection in 2 seconds\n",
      "Selected 417 important features. Too many to print...\n",
      "Total Time taken for featurewiz selection = 130 seconds\n",
      "Output contains a list of 417 important features and a train dataframe\n",
      "    Time taken to create entire pipeline = 139 second(s)\n",
      "Model trained successfully: LogisticRegression(C=0.01, solver='liblinear')\n",
      "wiz = FeatureWiz(verbose=1)\n",
      "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
      "        X_test_selected = wiz.transform(X_test)\n",
      "        wiz.features  ### provides a list of selected features ###            \n",
      "        \n",
      "featurewiz has selected 0.6 as the correlation limit. Change this limit to fit your needs...\n",
      "Skipping feature engineering since no feature_engg input...\n",
      "Skipping category encoding since no category encoders specified in input...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "    Loaded train data. Shape = (2924, 3739)\n",
      "    Some column names had special characters which were removed...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "No test data filename given...\n",
      "#######################################################################################\n",
      "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
      "#######################################################################################\n",
      "        No variables were removed since no ID or low-information variables found in data set\n",
      "Removing 0 columns from further processing since ID or low information variables\n",
      "#######################################################################################\n",
      "#####  Searching for Uncorrelated List Of Variables (SULOV) in 3738 features ############\n",
      "#######################################################################################\n",
      "    there are no null values in dataset...\n",
      "    Removing (3094) highly correlated variables:\n",
      "Completed SULOV. 644 features selected\n",
      "Time taken for SULOV method = 108 seconds\n",
      "Finally 644 vars selected after SULOV\n",
      "Converting all features to numeric before sending to XGBoost...\n",
      "    Number of booster rounds = 100\n",
      "            Time taken for regular XGBoost feature selection = 7 seconds\n",
      "            Time taken for regular XGBoost feature selection = 6 seconds\n",
      "            Time taken for regular XGBoost feature selection = 4 seconds\n",
      "            Time taken for regular XGBoost feature selection = 3 seconds\n",
      "            Time taken for regular XGBoost feature selection = 2 seconds\n",
      "    Completed XGBoost feature selection in 2 seconds\n",
      "Selected 417 important features. Too many to print...\n",
      "Total Time taken for featurewiz selection = 129 seconds\n",
      "Output contains a list of 417 important features and a train dataframe\n",
      "    Time taken to create entire pipeline = 138 second(s)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'penalty'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mexecute_feature_selection(corr_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m)\n\u001b[0;32m     39\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mexecute_preprocessing()\n\u001b[1;32m---> 40\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperform_grid_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhpers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m ppln \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mbuild_pipeline()\n\u001b[0;32m     43\u001b[0m me \u001b[38;5;241m=\u001b[39m behave_metrics\u001b[38;5;241m.\u001b[39mModelEvaluator(ppln,xval)\n",
      "File \u001b[1;32mc:\\Users\\dimza\\Desktop\\MachineLearningAutomator\\Helpers\\pipelines.py:193\u001b[0m, in \u001b[0;36mMLPipeline.train_model\u001b[1;34m(self, perform_grid_search, param_grid, cv)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_trainer\u001b[38;5;241m.\u001b[39mperform_grid_search(param_grid, cv\u001b[38;5;241m=\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39m auc_scorer)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model\u001b[38;5;241m.\u001b[39mget_params()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dimza\\Desktop\\MachineLearningAutomator\\Helpers\\pipelines.py:109\u001b[0m, in \u001b[0;36mModelTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m adjusted_hyperparameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__adjust_class_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier_hyperparameters)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Create and fit the pipeline\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madjusted_hyperparameters)\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclas\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'penalty'"
     ]
    }
   ],
   "source": [
    "pipeline_dict ={}\n",
    "scores_storage = {}\n",
    "params_dict = {}\n",
    "thresholds = {}\n",
    "skf = StratifiedKFold(n_splits=3, shuffle = True, random_state=10)\n",
    "skf.split(X_train, y_train)\n",
    "for cls, hp, nm in zip(classifiers, hypers, names):\n",
    "    if nm==\"Logistic Regression\":\n",
    "    # find optimal parameters\n",
    "        pipeline = pipelines.MLPipeline(X_train, y_train, cls, hp)\n",
    "        pipeline.execute_feature_selection(corr_limit=0.6)\n",
    "        pipeline.execute_preprocessing()\n",
    "        pipeline.train_model(perform_grid_search=True, param_grid=hp, cv=skf)\n",
    "        ppln = pipeline.build_pipeline()\n",
    "        params = pipeline.get_best_parameters()\n",
    "        params_dict.update({nm:params})\n",
    "\n",
    "        hpers = params # best parameters based on cv grid search\n",
    "\n",
    "        # Run the tests on best parameters\n",
    "        pipeline = pipelines.MLPipeline(X_train, y_train, cls, hpers)\n",
    "        pipeline.execute_feature_selection(corr_limit=0.6)\n",
    "        pipeline.execute_preprocessing()\n",
    "        pipeline.train_model()\n",
    "        ppln = pipeline.build_pipeline()\n",
    "        pipeline_dict.update({nm:ppln})\n",
    "\n",
    "        scores_storage.update({nm:{}})\n",
    "        thresholds.update({nm:None})\n",
    "\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n",
    "            xtrain = X_train.iloc[train_index,:]\n",
    "            ytrain = y_train.iloc[train_index]\n",
    "            xval = X_train.iloc[test_index,:]\n",
    "            yval = y_train.iloc[test_index]\n",
    "\n",
    "            pipeline = pipelines.MLPipeline(xtrain, ytrain, cls, hpers)\n",
    "            pipeline.execute_feature_selection(corr_limit=0.6)\n",
    "            pipeline.execute_preprocessing()\n",
    "            pipeline.train_model()\n",
    "            ppln = pipeline.build_pipeline()\n",
    "            \n",
    "            me = behave_metrics.ModelEvaluator(ppln,xval)\n",
    "            scores = me.evaluate()[\"y_test\"]\n",
    "\n",
    "            # Find optimal threshold\n",
    "            tho = behave_metrics.ThresholdOptimizer(ppln, xtrain, ytrain)\n",
    "            thresh = tho.find_optimal_threshold(metric_to_track=\"AUC\")\n",
    "            thresholds[nm].update({f\"fold_{i+1}\":{thresh}})\n",
    "\n",
    "            # Compute metrics on that threshold\n",
    "            mr = behave_metrics.Metrics(scores, yval)\n",
    "            mr.compute_metrics(threshold=thresh)\n",
    "            scores_dict = mr.get_scores() # the scores on the fold based on the best hyperparameters\n",
    "            scores_storage[nm].update({f\"fold_{i+1}\":scores_dict})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Whole X train and validation on the Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiz = FeatureWiz(verbose=1)\n",
      "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
      "        X_test_selected = wiz.transform(X_test)\n",
      "        wiz.features  ### provides a list of selected features ###            \n",
      "        \n",
      "featurewiz has selected 0.6 as the correlation limit. Change this limit to fit your needs...\n",
      "Skipping feature engineering since no feature_engg input...\n",
      "Skipping category encoding since no category encoders specified in input...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "    Loaded train data. Shape = (2924, 3739)\n",
      "    Some column names had special characters which were removed...\n",
      "#### Single_Label Binary_Classification problem ####\n",
      "No test data filename given...\n",
      "#######################################################################################\n",
      "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
      "#######################################################################################\n",
      "        No variables were removed since no ID or low-information variables found in data set\n",
      "Removing 0 columns from further processing since ID or low information variables\n",
      "#######################################################################################\n",
      "#####  Searching for Uncorrelated List Of Variables (SULOV) in 3738 features ############\n",
      "#######################################################################################\n",
      "    there are no null values in dataset...\n",
      "    Removing (3094) highly correlated variables:\n",
      "Completed SULOV. 644 features selected\n",
      "Time taken for SULOV method = 115 seconds\n",
      "Finally 644 vars selected after SULOV\n",
      "Converting all features to numeric before sending to XGBoost...\n",
      "    Number of booster rounds = 100\n",
      "            Time taken for regular XGBoost feature selection = 7 seconds\n",
      "            Time taken for regular XGBoost feature selection = 6 seconds\n",
      "            Time taken for regular XGBoost feature selection = 5 seconds\n",
      "            Time taken for regular XGBoost feature selection = 3 seconds\n",
      "            Time taken for regular XGBoost feature selection = 2 seconds\n",
      "    Completed XGBoost feature selection in 2 seconds\n",
      "Selected 417 important features. Too many to print...\n",
      "Total Time taken for featurewiz selection = 138 seconds\n",
      "Output contains a list of 417 important features and a train dataframe\n",
      "    Time taken to create entire pipeline = 147 second(s)\n",
      "Model trained successfully: LogisticRegression(C=0.01, solver='liblinear')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'pop from an empty set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold,val \u001b[38;5;129;01min\u001b[39;00m thresholds[nm]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     22\u001b[0m     cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 23\u001b[0m     meas \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m average_threshold \u001b[38;5;241m=\u001b[39m meas\u001b[38;5;241m/\u001b[39mcnt \u001b[38;5;28;01mif\u001b[39;00m cnt\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Compute metrics on that threshold\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pop from an empty set'"
     ]
    }
   ],
   "source": [
    "pipeline_dict_inf = {}\n",
    "params_inf= {}\n",
    "scores_inf = {}\n",
    "for cls, hp, nm in zip(classifiers, hypers, names):\n",
    "    if nm==\"Logistic Regression\":\n",
    "        hpers = params_dict[nm]\n",
    "        pipeline = pipelines.MLPipeline(X_train, y_train, cls, hpers)\n",
    "        pipeline.execute_feature_selection(corr_limit=0.6)\n",
    "        pipeline.execute_preprocessing()\n",
    "        pipeline.train_model(perform_grid_search=False)\n",
    "        ppln = pipeline.build_pipeline()\n",
    "        pipeline_dict_inf.update({nm:ppln})\n",
    "        params_inf.update({nm:params})\n",
    "\n",
    "        me = behave_metrics.ModelEvaluator(ppln,xval)\n",
    "        scores = me.evaluate()[\"y_test\"]\n",
    "\n",
    "        # Set optimal threshold as the average across Folds\n",
    "        cnt = 0\n",
    "        meas = 0\n",
    "        for fold,val in thresholds[nm].items():\n",
    "            cnt += 1\n",
    "            meas += val\n",
    "        average_threshold = meas/cnt if cnt!=0 else 0.5\n",
    "\n",
    "        # Compute metrics on that threshold\n",
    "        mr = behave_metrics.Metrics(scores, yval)\n",
    "        mr.compute_metrics(threshold=average_threshold)\n",
    "        scores_dict = mr.get_scores() # the scores on the fold based on the best hyperparameters\n",
    "        scores_inf.update({nm:scores_dict})\n",
    "\n",
    "        # Perform Shap\n",
    "        sp = shap_module.ShapAnalysis(X_val = X_test, pipeline_module = ppln, features=pipeline.selected_features) \n",
    "        shap_values,features = sp.perform_shap(), pipeline.selected_features\n",
    "        sp.plot_shap_values(model_name=nm, path=r\"./Materials/Shap_Features\")\n",
    "\n",
    "# display roc curves\n",
    "roc = behave_metrics.ROCCurveEvaluator(pipeline_dict_inf,X_test=X_test, y_true=y_test)\n",
    "roc.evaluate_models()\n",
    "roc.plot_roc_curves(save_path=r\"./Materials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fold_1': set(), 'fold_2': {0.7200000000000001}, 'fold_3': {0.67}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds[nm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ____ End of Code ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_csv = r\"Data\\UC2_retrospective\\ProCancer_UC2_N4_ADC_DL_PZ_radio.csv\"\n",
    "problematic = r\"D:\\dimza\\EXPERIMENTS\\ProstateCancerClinicalSignificance_UC2\\UC2_prospective_and_retrospective\\Borderline_cases_to_remove.csv\"\n",
    "jenn = r\"Data\\data_characteristics.csv\"\n",
    "\n",
    "exl = [\"study_uid\",\n",
    "\"number_of_series\",\n",
    "\"pi_rads\"]\n",
    "\n",
    "rdc = RadiomicsClean(path_to_csv)\n",
    "rdc.clean_borderlines(problematic)\n",
    "rdc.drop_columns_from_df(excluded_columns = exl)\n",
    "#rdc.create_target()\n",
    "rdc.create_target_based_on_csv(target_csv=jenn)\n",
    "rdc.keep_zone_location(zone=\"PZ\")\n",
    "#rdc.add_suffix(\"T2\")\n",
    "features = rdc.get_processed()\n",
    "kalliatakis = X_test.iloc[:,0:2]\n",
    "kalliatakis[\"y_pred\"] = scores\n",
    "kalliatakis[\"y_true\"] = y_test\n",
    "kalliatakis.drop(columns=[\"gradient_firstorder_10Percentile_T2\",\"gradient_firstorder_90Percentile_T2\"],inplace=True)\n",
    "df1 = features.loc[kalliatakis.index,[\"provided_by\", \"manufacturer\", \"manufacturer_model_name\"]]\n",
    "kalliatakis = pd.concat([df1,kalliatakis], axis=1)\n",
    "kalliatakis = kalliatakis[kalliatakis.provided_by.isin([\"QUIRONSALUD\",\"UNIPI\", \"HULAFE\", \"FPO\", \"IDIBGI\"])]\n",
    "kalliatakis[\"Country\"] = \"Spain\"\n",
    "kalliatakis[\"Country\"][kalliatakis.provided_by.isin([\"UNIPI\", \"FPO\"])] = \"Italy\"\n",
    "kalliatakis.to_csv(\"ClinicalSignificant_ProstateCancer.csv\")\n",
    "kalliatakis[kalliatakis.provided_by.isin([\"QUIRONSALUD\",\"UNIPI\", \"HULAFE\", \"FPO\", \"IDIBGI\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = [\n",
    "    r\"Data\\UC2_prospespective\\ProCancer_UC2_N4_T2_DL_PZ_radio.csv\",\n",
    "    r\"Data\\UC2_prospespective\\ProCancer_UC2_N4_ADC_DL_PZ_radio.csv\",\n",
    "    r\"Data\\UC2_prospespective\\ProCancer_UC2_N4_DWI_DL_PZ_radio.csv\",\n",
    "]\n",
    "\n",
    "mpp = MultiParamProcessor(\n",
    "    csv_paths = csv_paths,\n",
    "    problematic = problematic\n",
    ")\n",
    "mpp.iteration(\n",
    "    suffices=[\"T2\", \"ADC\", \"DWI\"],\n",
    "    columns_to_exclude= exl\n",
    ")\n",
    "\n",
    "dfs = mpp.get_dfs()\n",
    "prospe = pd.concat([dfs[0], dfs[1], dfs[2]], axis = 1)\n",
    "X_test = prospe.drop('Target', axis=1)  # Drop the 'Target' column for X_train\n",
    "y_test = prospe['Target']\n",
    "\n",
    "evaluator = ModelEvaluator(pipeline, X_test)\n",
    "\n",
    "# Evaluate the model on validation and test datasets\n",
    "evaluation_results = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quadible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
