{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helpers import pipelines\n",
    "from Helpers import behave_metrics\n",
    "from Helpers import shap_module\n",
    "from Helpers import MetricsReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      " LR is starting \n",
      " --------------------\n"
     ]
    }
   ],
   "source": [
    "nm = \"LR\"\n",
    "print(\"-------------------- \\n\", f\"{nm} is starting \\n\", \"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "#from Helpers.radiomics_setup import RadiomicsClean, MultiParamProcessor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import yaml\n",
    "# Define the hyperparameters for each model\n",
    "hyperparameters_models_grid = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"classifier\": LogisticRegression,\n",
    "        \"params\": {\n",
    "            'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "            'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "            'solver': ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "            'max_iter': [50, 100, 200, 300, 500],\n",
    "            'l1_ratio': [0, 0.1, 0.2, 0.5, 0.7, 0.9, 1]\n",
    "        },\n",
    "        \"default_params\": {}\n",
    "    },\n",
    "    \"Support Vector Machines\": {\n",
    "        \"classifier\": SVC,\n",
    "        \"params\": {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "            'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'degree': [2, 3, 4, 5],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'coef0': [0, 0.1, 0.5, 1, 2],\n",
    "            'shrinking': [True, False],\n",
    "            'tol': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "            'probability': [True]\n",
    "        },\n",
    "        \"default_params\": {}\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"classifier\": RandomForestClassifier,\n",
    "        \"params\": {\n",
    "            'n_estimators': [20, 50, 100, 200, 300, 400, 500],\n",
    "            'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "            'max_depth': [None, 2, 4, 6, 10, 20, 30, 40, 50],\n",
    "            'min_samples_split': [2, 5, 10, 20],\n",
    "            'min_samples_leaf': [1, 2, 4, 10],\n",
    "            'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "            'bootstrap': [True, False]\n",
    "        },\n",
    "        \"default_params\": {}\n",
    "    },\n",
    "    \"Stochastic Gradient Descent\": {\n",
    "        \"classifier\": SGDClassifier,\n",
    "        \"params\": {\n",
    "            'loss': ['log', 'modified_huber'],\n",
    "            'penalty': ['none', 'l2', 'l1', 'elasticnet'],\n",
    "            'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n",
    "            'l1_ratio': [0.0, 0.15, 0.5, 0.85, 1.0],\n",
    "            'fit_intercept': [True, False],\n",
    "            'max_iter': [1000, 2000, 3000],\n",
    "            'tol': [1e-3, 1e-4, 1e-5],\n",
    "            'shuffle': [True, False],\n",
    "            'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "            'eta0': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "            'power_t': [0.25, 0.5, 0.75],\n",
    "            'early_stopping': [True, False],\n",
    "            'validation_fraction': [0.1, 0.2, 0.3],\n",
    "            'n_iter_no_change': [5, 10, 20],\n",
    "            'average': [False, True]\n",
    "        },\n",
    "        \"default_params\": {}\n",
    "    },\n",
    "    \"Multi-Layer Neural Network\": {\n",
    "        \"classifier\": MLPClassifier,\n",
    "        \"params\": {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
    "            'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "            'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "            'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "            'max_iter': [200, 300, 500, 1000],\n",
    "            'shuffle': [True, False],\n",
    "            'tol': [1e-4, 1e-3, 1e-2],\n",
    "            'momentum': [0.9, 0.95, 0.99],\n",
    "            'nesterovs_momentum': [True, False],\n",
    "            'early_stopping': [True, False],\n",
    "            'beta_1': [0.9, 0.95, 0.99],\n",
    "            'beta_2': [0.999, 0.9995, 0.9999],\n",
    "            'epsilon': [1e-8, 1e-7, 1e-6]\n",
    "        },\n",
    "        \"default_params\": {}\n",
    "    },\n",
    "    \"Decision Trees\": {\n",
    "        \"classifier\": DecisionTreeClassifier,\n",
    "        \"params\": {\n",
    "            'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_depth': [None, 2, 3, 5, 10, 20, 30, 40, 50],\n",
    "            'min_samples_split': [2, 5, 10, 20],\n",
    "            'min_samples_leaf': [1, 2, 4, 10],\n",
    "            'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "            'max_leaf_nodes': [None, 2, 4, 10, 20, 30, 40, 50]\n",
    "        },\n",
    "        \"default_params\": {}\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"classifier\": XGBClassifier,\n",
    "        \"params\": {\n",
    "            'n_estimators': [3, 5, 10, 50, 100, 200, 300, 400, 500],\n",
    "            'max_depth': [3, 4, 5, 6, 7, 8, 10],\n",
    "            'learning_rate': [0.001, 0.01, 0.1, 0.3, 0.5],\n",
    "            'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'gamma': [0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "            'reg_alpha': [0, 0.01, 0.1, 1, 10, 100],\n",
    "            'reg_lambda': [0.01, 0.1, 1, 10, 100]\n",
    "        },\n",
    "        \"default_params\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "yaml_file = os.path.join(\"./input_data\", \"machine_learning_parameters.yaml\")\n",
    "with open(yaml_file, 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "\n",
    "CORRELATION_LIMIT = params[\"Correlation Limit\"]\n",
    "\n",
    "NUMBER_OF_FOLDS = params[\"number_of_k_folds\"]\n",
    "\n",
    "if params[\"apply_grid_search\"][\"type\"][\"Randomized\"]:\n",
    "    HP_TYPE = \"Randomized\"\n",
    "else:\n",
    "    HP_TYPE = \"Exhaustive\"\n",
    "\n",
    "names = []\n",
    "classifiers = []\n",
    "hypers = []\n",
    "\n",
    "GRID_SEARCH_ENABLING = params[\"apply_grid_search\"][\"enabled\"]\n",
    "\n",
    "# Iterate over the machine learning models specified in the YAML file\n",
    "\n",
    "for model_name, is_enabled in params[\"Machine Learning Models\"].items():\n",
    "    if is_enabled:\n",
    "        model_info = hyperparameters_models_grid.get(model_name)\n",
    "        if GRID_SEARCH_ENABLING:\n",
    "            names.append(model_name)\n",
    "            classifiers.append(model_info[\"classifier\"])\n",
    "            hypers.append(model_info[\"params\"])\n",
    "        else:\n",
    "            names.append(model_name)\n",
    "            classifiers.append(model_info[\"classifier\"])\n",
    "            hypers.append(model_info[\"default_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Support Vector Machines',\n",
       " 'Stochastic Gradient Descent',\n",
       " 'Multi-Layer Neural Network',\n",
       " 'Decision Trees',\n",
       " 'XGBoost']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"Machine Learning Models\"]\n",
    "for k,v in params[\"Machine Learning Models\"].keys():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"number_of_k_folds\"]\n",
    "params[\"apply_grid_search\"][\"type\"][\"randomized\"]\n",
    "\n",
    "#params[\"Machine Learning Models\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "from Helpers.radiomics_setup import RadiomicsClean, MultiParamProcessor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = [\n",
    "    r\"Data\\UC2_retrospective\\ProCancer_UC2_N4_T2_DL_PZ_radio.csv\",\n",
    "    r\"Data\\UC2_retrospective\\ProCancer_UC2_N4_ADC_DL_PZ_radio.csv\",\n",
    "    r\"Data\\UC2_retrospective\\ProCancer_UC2_N4_DWI_DL_PZ_radio.csv\",\n",
    "]\n",
    "\n",
    "exl = [\"study_uid\",\n",
    "\"number_of_series\",\n",
    "\"provided_by\",\n",
    "\"manufacturer\",\n",
    "\"manufacturer_model_name\",\n",
    "\"pi_rads\"]\n",
    "\n",
    "problematic = r\"D:\\dimza\\EXPERIMENTS\\ProstateCancerClinicalSignificance_UC2\\UC2_prospective_and_retrospective\\Borderline_cases_to_remove.csv\"\n",
    "jenn = r\"Data\\data_characteristics.csv\"\n",
    "\n",
    "mpp = MultiParamProcessor(\n",
    "    csv_paths = csv_paths,\n",
    "    problematic = problematic\n",
    ")\n",
    "mpp.iteration(\n",
    "    suffices=[\"T2\", \"ADC\", \"DWI\"],\n",
    "    columns_to_exclude= exl,\n",
    "    target_csv=jenn\n",
    ")\n",
    "\n",
    "dfs = mpp.get_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test\n",
    "parent_folder = \"./input_data\"\n",
    "train = pd.concat([X_train, y_train], axis = 1)\n",
    "test = pd.concat([X_test, y_test], axis = 1)\n",
    "train.to_csv(os.path.join(parent_folder,\"Train.csv\"))\n",
    "test.to_csv(os.path.join(parent_folder,\"Test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retro = pd.concat([dfs[0], dfs[1], dfs[2]], axis = 1)\n",
    "X = retro.drop('Target', axis=1)  # Drop the 'Target' column for X_train\n",
    "y = retro['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helpers import pipelines\n",
    "from Helpers import behave_metrics\n",
    "from Helpers import shap_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = \"./input_data\"\n",
    "train = pd.read_csv(os.path.join(parent_folder,\"Train.csv\"), index_col=\"patient_id\")\n",
    "test = pd.read_csv(os.path.join(parent_folder,\"Test.csv\"),index_col=\"patient_id\")\n",
    "X_train = train.drop('Target', axis=1)  # Drop the 'Target' column for X_train\n",
    "y_train = train['Target']\n",
    "X_test = test.drop('Target', axis=1)  # Drop the 'Target' column for X_train\n",
    "y_test = test['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hyperparameters = {'C': 1.0, 'penalty': 'l2'}\n",
    "svm_hyperparameters = {'C': 1.0, 'kernel': 'rbf', 'probability':True}\n",
    "xgb_hyperparameters = {'n_estimators': 100}\n",
    "ada_hyperparameters = {'n_estimators': 50}\n",
    "rf_hyperparameters = {'n_estimators': 100}\n",
    "dt_hyperparameters = {'criterion': 'gini'}\n",
    "\n",
    "names = [\"Logistic Regression\", \"Support Vector Machines\", \"Random Forest\", \"AdaBoost\", \"Decision Trees\", \"XGBoost\"]\n",
    "classifiers = [LogisticRegression, SVC, RandomForestClassifier, AdaBoostClassifier, DecisionTreeClassifier, XGBClassifier]\n",
    "hypers = [lr_hyperparameters , svm_hyperparameters, rf_hyperparameters, ada_hyperparameters, dt_hyperparameters, xgb_hyperparameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hyperparameters = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': [None, 'l2'],\n",
    "    'solver': ['lbfgs','liblinear', 'saga','newton-cholesky'],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "svm_hyperparameters = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'probability':[True],\n",
    "}\n",
    "\n",
    "rf_hyperparameters = {\n",
    "    'n_estimators': [2,4,6,10,20,50],\n",
    "    'max_depth': [None, 1,2,3, 4,6,8, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "ada_hyperparameters = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    \"n_jobs\":[-1]\n",
    "}\n",
    "\n",
    "dt_hyperparameters = {\n",
    "    'max_depth': [None, 2,3, 5, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "xgb_hyperparameters = {\n",
    "    'n_estimators': [3,5,10, 50],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [None, 1,2, 3, 6, 10, 20],\n",
    "    'subsample': [0.5, 0.7, 1],\n",
    "    'colsample_bytree': [0.5, 0.7, 1],\n",
    "}\n",
    "hypers = [lr_hyperparameters , svm_hyperparameters, rf_hyperparameters, ada_hyperparameters, dt_hyperparameters, xgb_hyperparameters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV-based on stratified K-Fold split and measurements on each fodl seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict ={}\n",
    "scores_storage = {}\n",
    "params_dict = {}\n",
    "thresholds = {}\n",
    "# to add the automated k-fold selector based on the number of y\n",
    "skf = StratifiedKFold(n_splits=3, shuffle = True, random_state=10)\n",
    "for cls, hp, nm in zip(classifiers, hypers, names):\n",
    "    if nm==\"Decision Trees\":\n",
    "    # find optimal parameters\n",
    "        pipeline = pipelines.MLPipeline(X_train, y_train, cls, hp)\n",
    "        pipeline.execute_feature_selection(corr_limit=0.6)\n",
    "        pipeline.execute_preprocessing()\n",
    "        pipeline.train_model(perform_grid_search=True, param_grid=hp, cv=skf)\n",
    "        ppln = pipeline.build_pipeline()\n",
    "        params = pipeline.get_best_parameters()\n",
    "        params_dict.update({nm:params})\n",
    "\n",
    "        hpers = params # best parameters based on cv grid search\n",
    "\n",
    "        scores_storage_algo = {}\n",
    "        thresholds_algo = {}\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n",
    "            xtrain = X_train.iloc[train_index,:]\n",
    "            ytrain = y_train.iloc[train_index]\n",
    "            xval = X_train.iloc[test_index,:]\n",
    "            yval = y_train.iloc[test_index]\n",
    "\n",
    "            pipeline = pipelines.MLPipeline(xtrain, ytrain, cls, hpers)\n",
    "            pipeline.execute_feature_selection(corr_limit=0.6)\n",
    "            pipeline.execute_preprocessing()\n",
    "            pipeline.train_model()\n",
    "            ppln = pipeline.build_pipeline()\n",
    "            \n",
    "            me = behave_metrics.ModelEvaluator(ppln,xval)\n",
    "            scores = me.evaluate()[\"y_test\"]\n",
    "\n",
    "            # Find optimal threshold\n",
    "            tho = behave_metrics.ThresholdOptimizer(ppln, xtrain, ytrain)\n",
    "            thresh = tho.find_optimal_threshold(metric_to_track=\"AUC\")\n",
    "            thresholds_algo.update({f\"fold_{i+1}\":thresh})\n",
    "\n",
    "            # Compute metrics on that threshold\n",
    "            mr = behave_metrics.Metrics(scores, yval)\n",
    "            mr.compute_metrics(threshold=thresh)\n",
    "            scores_dict = mr.get_scores() # the scores on the fold based on the best hyperparameters\n",
    "            scores_storage_algo.update({f\"fold_{i+1}\":scores_dict})\n",
    "        scores_storage.update({nm:scores_storage_algo})\n",
    "        thresholds.update({nm:thresholds_algo})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helpers import MetricsReport\n",
    "MetricsReport.summary_results_excel(scores_storage, file = \"k_fold_results\", conf_matrix_name=\"Internal_K_fold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Whole X train and validation on the Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict_inf = {}\n",
    "params_inf= {}\n",
    "scores_inf = {}\n",
    "for cls, hp, nm in zip(classifiers, hypers, names):\n",
    "    if nm==\"Decision Trees\":\n",
    "        hpers = params_dict[nm]\n",
    "        pipeline = pipelines.MLPipeline(X_train, y_train, cls, hpers)\n",
    "        pipeline.execute_feature_selection(corr_limit=0.6)\n",
    "        pipeline.execute_preprocessing()\n",
    "        pipeline.train_model(perform_grid_search=False)\n",
    "        ppln = pipeline.build_pipeline()\n",
    "        pipeline_dict_inf.update({nm:ppln})\n",
    "        params_inf.update({nm:params})\n",
    "\n",
    "        me = behave_metrics.ModelEvaluator(ppln,X_test)\n",
    "        scores = me.evaluate()[\"y_test\"]\n",
    "\n",
    "        # Set optimal threshold as the average across Folds\n",
    "        cnt = 0\n",
    "        meas = 0\n",
    "        for fold,val in thresholds[nm].items():\n",
    "            cnt += 1\n",
    "            meas += val\n",
    "        average_threshold = meas/cnt if cnt!=0 else 0.5\n",
    "\n",
    "        # Compute metrics on that threshold\n",
    "        mr = behave_metrics.Metrics(scores, y_test)\n",
    "        mr.compute_metrics(threshold=average_threshold)\n",
    "        scores_dict = mr.get_scores() # the scores on the fold based on the best hyperparameters\n",
    "        scores_inf.update({nm:scores_dict})\n",
    "\n",
    "        # Perform Shap\n",
    "        sp = shap_module.ShapAnalysis(X_val = X_test, pipeline_module = ppln, features=pipeline.selected_features) \n",
    "        shap_values,features = sp.perform_shap(), pipeline.selected_features\n",
    "\n",
    "        os.mkdir(os.path.join(\"Materials\", \"Shap_Features\"), exist_ok=True)\n",
    "        path_shap = os.path.join(\"Materials\", \"Shap_Features\")\n",
    "        sp.plot_shap_values(model_name=nm, path= path_shap)\n",
    "\n",
    "# display roc curves\n",
    "roc = behave_metrics.ROCCurveEvaluator(pipeline_dict_inf,X_test=X_test, y_true=y_test)\n",
    "roc.evaluate_models()\n",
    "roc.plot_roc_curves(save_path=r\"./Materials\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetricsReport.external_summary(scores_inf, file = \"test_results\", conf_matrix_name=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ____ End of Code ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_csv = r\"Data\\UC2_retrospective\\ProCancer_UC2_N4_ADC_DL_PZ_radio.csv\"\n",
    "problematic = r\"D:\\dimza\\EXPERIMENTS\\ProstateCancerClinicalSignificance_UC2\\UC2_prospective_and_retrospective\\Borderline_cases_to_remove.csv\"\n",
    "jenn = r\"Data\\data_characteristics.csv\"\n",
    "\n",
    "exl = [\"study_uid\",\n",
    "\"number_of_series\",\n",
    "\"pi_rads\"]\n",
    "\n",
    "rdc = RadiomicsClean(path_to_csv)\n",
    "rdc.clean_borderlines(problematic)\n",
    "rdc.drop_columns_from_df(excluded_columns = exl)\n",
    "#rdc.create_target()\n",
    "rdc.create_target_based_on_csv(target_csv=jenn)\n",
    "rdc.keep_zone_location(zone=\"PZ\")\n",
    "#rdc.add_suffix(\"T2\")\n",
    "features = rdc.get_processed()\n",
    "kalliatakis = X_test.iloc[:,0:2]\n",
    "kalliatakis[\"y_pred\"] = scores\n",
    "kalliatakis[\"y_true\"] = y_test\n",
    "kalliatakis.drop(columns=[\"gradient_firstorder_10Percentile_T2\",\"gradient_firstorder_90Percentile_T2\"],inplace=True)\n",
    "df1 = features.loc[kalliatakis.index,[\"provided_by\", \"manufacturer\", \"manufacturer_model_name\"]]\n",
    "kalliatakis = pd.concat([df1,kalliatakis], axis=1)\n",
    "kalliatakis = kalliatakis[kalliatakis.provided_by.isin([\"QUIRONSALUD\",\"UNIPI\", \"HULAFE\", \"FPO\", \"IDIBGI\"])]\n",
    "kalliatakis[\"Country\"] = \"Spain\"\n",
    "kalliatakis[\"Country\"][kalliatakis.provided_by.isin([\"UNIPI\", \"FPO\"])] = \"Italy\"\n",
    "kalliatakis.to_csv(\"ClinicalSignificant_ProstateCancer.csv\")\n",
    "kalliatakis[kalliatakis.provided_by.isin([\"QUIRONSALUD\",\"UNIPI\", \"HULAFE\", \"FPO\", \"IDIBGI\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = [\n",
    "    r\"Data\\UC2_prospespective\\ProCancer_UC2_N4_T2_DL_PZ_radio.csv\",\n",
    "    r\"Data\\UC2_prospespective\\ProCancer_UC2_N4_ADC_DL_PZ_radio.csv\",\n",
    "    r\"Data\\UC2_prospespective\\ProCancer_UC2_N4_DWI_DL_PZ_radio.csv\",\n",
    "]\n",
    "\n",
    "mpp = MultiParamProcessor(\n",
    "    csv_paths = csv_paths,\n",
    "    problematic = problematic\n",
    ")\n",
    "mpp.iteration(\n",
    "    suffices=[\"T2\", \"ADC\", \"DWI\"],\n",
    "    columns_to_exclude= exl\n",
    ")\n",
    "\n",
    "dfs = mpp.get_dfs()\n",
    "prospe = pd.concat([dfs[0], dfs[1], dfs[2]], axis = 1)\n",
    "X_test = prospe.drop('Target', axis=1)  # Drop the 'Target' column for X_train\n",
    "y_test = prospe['Target']\n",
    "\n",
    "evaluator = ModelEvaluator(pipeline, X_test)\n",
    "\n",
    "# Evaluate the model on validation and test datasets\n",
    "evaluation_results = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quadible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
